# -*- coding: utf-8 -*-
"""World Happiness Report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11dhbVZhnegEvkjw50F1sQFbEd3vn5gwE

# World Happiness Report — ETL & QC (2015–2024)
מטרת המחברת: איחוד הקבצים השנתיים, יישור סכימה, בדיקות איכות (כפילויות/חסרים/טווחים), ושמירת קובץ מאוחד נקי לשימוש ב-Power BI ובמודלים.

#Imports בסיסיים
 ייבוא ספריות והגדרות בסיס
"""

import pandas as pd, numpy as np, glob, os, textwrap, random
import re
from IPython.display import display  # אופציונלי לנוחות
from google.colab import files

pd.set_option('display.max_colwidth', 120)

RANDOM_SEED = 42  # לשחזוריות
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
EXPECTED_COLS = [
    "Country","Year","Ladder_Score","GDP_per_capita","Social_support",
    "Healthy_life_expectancy","Freedom","Generosity","Corruption"
]
print("Imports OK. Ready for ETL & QC.")

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q --upgrade yellowbrick statsmodels

import numpy as np, pandas as pd,glob, os, textwrap, matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
from yellowbrick.cluster import KElbowVisualizer
import statsmodels.api as sm

"""# העלאת קבצים משנים 2015-2024

"""

uploaded = files.upload()
print("קבצים שעלו:")
for k in uploaded.keys():
    print(" -", k)

#  תא 3: בדיקת קבצים שנתיים
import glob, os

DATA_DIR = "/content"  # כאן נשמרים הקבצים שהעלית עם files.upload
YEARLY_GLOB = os.path.join(DATA_DIR, "[2][0][1-2][0-9].csv")
CLEAN_PATH  = os.path.join(DATA_DIR, "world_happiness_2015_2024_clean.csv")

yearly_files = sorted(glob.glob(YEARLY_GLOB))
print("נמצאו", len(yearly_files), "קבצים שנתיים.")
print("דוגמאות:", [os.path.basename(p) for p in yearly_files[:]])

assert len(yearly_files) > 0, "לא נמצאו קבצים שנתיים, בדקי שהעלית את כולם."

"""מבט מהיר על כל קובץ (שמות עמודות/גדלים)

"""

#  מבט מהיר על כל קובץ לפני איחוד
per_file_info = []

for fname in sorted(uploaded.keys()):
    try:
        df = pd.read_csv(fname)
    except UnicodeDecodeError:
        df = pd.read_csv(fname, encoding='latin-1')
    per_file_info.append({
        "file": fname,
        "rows": df.shape[0],
        "cols": df.shape[1],
        # נציג רק 5 העמודות הראשונות לדוגמא
        "columns (sample)": list(df.columns[:5])
    })

pd.DataFrame(per_file_info)

"""# הגדרות ETL: עמודות קנוניות + נרמול שמות + מיפוי וריאציות"""

# 1) שמות עמודות קנוניים שנרצה בטבלה המאוחדת
canonical_cols = [
    "Country","Year","Ladder_Score","GDP_per_capita","Social_support",
    "Healthy_life_expectancy","Freedom","Generosity","Corruption"
]

# 2) נרמול שם עמודה: אותיות קטנות, הסרת סימני פיסוק, רווח אחד
def normalize_colname(c: str) -> str:
    c = c.strip().lower()
    c = re.sub(r'[^a-z0-9]+', ' ', c)     # כל מה שלא אות/ספרה → רווח
    c = re.sub(r'\s+', ' ', c).strip()    # רווחים כפולים → יחיד
    return c

# 3) מיפוי וריאציות → לשמות הקנוניים (מכסה גרסאות “ישנות”/“חדשות” של הדוח)
variants_norm = {
    # מזהים
    "country": "Country",
    "country name": "Country",
    "country or region": "Country",
    "year": "Year",

    # ציון האושר (Ladder_Score)
    "happiness score": "Ladder_Score",
    "score": "Ladder_Score",
    "ladder score": "Ladder_Score",
    "life ladder": "Ladder_Score",

    # תוצר לנפש (GDP_per_capita) — לעתים בלוג/לוגד
    "economy gdp per capita": "GDP_per_capita",
    "gdp per capita": "GDP_per_capita",
    "log gdp per capita": "GDP_per_capita",
    "logged gdp per capita": "GDP_per_capita",

    # תמיכה חברתית
    "family": "Social_support",
    "social support": "Social_support",

    # תוחלת חיים בריאה
    "health life expectancy": "Healthy_life_expectancy",
    "healthy life expectancy": "Healthy_life_expectancy",
    "healthy life expectancy at birth": "Healthy_life_expectancy",

    # חירות
    "freedom": "Freedom",
    "freedom to make life choices": "Freedom",

    # נדיבות
    "generosity": "Generosity",

    # שחיתות
    "trust government corruption": "Corruption",
    "perceptions of corruption": "Corruption",
}

# 4) קריאת CSV סובלנית (אם יש בעיות קידוד)
def read_csv_best_effort(path: str) -> pd.DataFrame:
    try:
        return pd.read_csv(path)
    except UnicodeDecodeError:
        return pd.read_csv(path, encoding="latin-1")

# 5) פונקציית איחוד שמות + זיהוי סכימה (“modern” ~ 2020+)
def harmonize_one(df: pd.DataFrame, year_from_name=None) -> pd.DataFrame:
    # נרמול שמות מקור
    orig_cols     = list(df.columns)
    norm_map      = {c: normalize_colname(c) for c in df.columns}  # מקור -> מנורמל
    norm_to_orig  = {v: k for k, v in norm_map.items()}            # מנורמל -> מקור
    norm_names    = set(norm_map.values())

    # זיהוי סכימה מודרנית לפי שמות עמודות טיפוסיים אחרי 2020
    is_modern = any(k in norm_names for k in [
        "life ladder", "log gdp per capita", "healthy life expectancy at birth", "perceptions of corruption"
    ])
    schema_label = "modern" if is_modern else "legacy"

    # בניית מפת שינוי שמות לפי הווריאציות
    rename_map = {}
    for low_norm, target in variants_norm.items():
        if low_norm in norm_to_orig:
            rename_map[norm_to_orig[low_norm]] = target

    # אכיפה: אם עמודה כבר קנונית (אך כתובה אחרת), לאחד לשם הקנוני
    for c in orig_cols:
        norm = normalize_colname(c)
        for cc in canonical_cols:
            if norm == normalize_colname(cc):
                rename_map[c] = cc

    # שינוי שמות
    df2 = df.rename(columns=rename_map).copy()

    # הוספת Year אם חסרה — נלקחת משם הקובץ
    if "Year" not in df2.columns:
        df2["Year"] = year_from_name

    # ולידציה בסיסית
    if "Country" not in df2.columns:
        raise ValueError("עמודת 'Country' חסרה אחרי איחוד שמות. בדקי את המיפוי/שמות בקובץ.")
    if "Year" not in df2.columns:
        raise ValueError("עמודת 'Year' חסרה אחרי איחוד שמות. העבירי year_from_name או ודאי שהיא מופיעה בקובץ.")

    # המרות טיפוסים: Year למספרי, מדדים ל-float, Country לניקוי רווחים
    for c in df2.columns:
        if c not in ("Country", "Year"):
            df2[c] = pd.to_numeric(df2[c], errors="coerce")
    df2["Year"]    = pd.to_numeric(df2["Year"], errors="coerce").astype("Int64")
    df2["Country"] = df2["Country"].astype(str).str.strip()

    # ציון סוג הסכימה (לתיעוד וטרייסביליות)
    df2["Schema"] = schema_label

    # החזרת עמודות מסודרות — רק הקנוניות + Schema
    cols_order = ["Country", "Year"] + [c for c in canonical_cols if c not in ("Country","Year")] + ["Schema"]
    cols_order = [c for c in cols_order if c in df2.columns]
    df2 = df2[cols_order].copy()

    # דוח מיני: אם חסרות עמודות קנוניות בקובץ הזה
    missing_here = [c for c in canonical_cols if c not in df2.columns]
    if missing_here:
        print(f"[אזהרה] עמודות קנוניות חסרות בקובץ זה: {missing_here}")

    return df2

# 6) המרת טיפוסים בטוחה למסגרת המאוחדת (נשתמש אחרי איחוד)
def coerce_types(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if "Year" in df.columns:
        df["Year"] = pd.to_numeric(df["Year"], errors="coerce").astype("Int64")
    num_cols = [c for c in canonical_cols if c not in ["Country","Year"] and c in df.columns]
    for c in num_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    if "Country" in df.columns:
        df["Country"] = df["Country"].astype(str).str.strip()
    return df

"""# איחוד כל השנים לטבלה אחת

"""

# === תא 5: איחוד קבצים שנתיים ל-DataFrame אחד נקי ===
import re

parts, summaries = [], []

for fname in sorted(uploaded.keys()):
    try:
        raw = pd.read_csv(fname)
        if raw.shape[1] == 1 and ';' in str(raw.iloc[0,0]):
            raw = pd.read_csv(fname, sep=';')
    except UnicodeDecodeError:
        raw = pd.read_csv(fname, encoding='latin-1')

    # חילוץ השנה משם הקובץ (למקרה שאין Year בפנים)
    m = re.search(r'(\d{4})', fname)
    yr = int(m.group(1)) if m else None

    cleaned = harmonize_one(raw, year_from_name=yr)
    parts.append(cleaned)

    summaries.append({
        "file": fname,
        "rows_before": raw.shape[0], "cols_before": raw.shape[1],
        "rows_after": cleaned.shape[0], "cols_after": cleaned.shape[1],
        "after_columns": list(cleaned.columns)
    })

# איחוד כל השנים למסגרת אחת
combined = pd.concat(parts, ignore_index=True, sort=False)

# סידור עמודות לפי הסכמה הקנונית
cols_order = ["Country","Year","Ladder_Score","GDP_per_capita","Social_support",
              "Healthy_life_expectancy","Freedom","Generosity","Corruption","Schema"]
combined = combined[[c for c in cols_order if c in combined.columns]]

# ניקוי אחרון: הסרת רשומות בלי מדינה/ציון, מיון, הסרת כפילויות מדינה–שנה
clean = combined.dropna(subset=["Country","Ladder_Score"]).copy()
clean["Year"] = pd.to_numeric(clean["Year"], errors="coerce").astype("Int64")
clean = clean.sort_values(["Country","Year"]).drop_duplicates(["Country","Year"])

# בדיקות בסיסיות
rows, cols = clean.shape
print(f"מספר שורות: {rows}")
print(f"מספר עמודות: {cols}\n")

missing_per_col = clean.isna().sum().sort_values(ascending=False)
print("כמה ערכים חסרים בכל עמודה:\n", missing_per_col, "\n")

years = sorted([int(y) for y in clean["Year"].dropna().unique().tolist()])
print("שנים זמינות:", years)
print("\nכמה רשומות לכל שנה:")
print(clean.groupby("Year").size())

"""## טיפול חכם בחוסרים
מילוי ליניארי לאורך שנים פר־מדינה
"""

# טיפול חכם בחוסרים (גרסה יציבה וללא DeprecationWarning)
num_cols = ["Ladder_Score","GDP_per_capita","Social_support",
            "Healthy_life_expectancy","Freedom","Generosity","Corruption"]
num_cols = [c for c in num_cols if c in clean.columns]

# 1) אינטרפולציה ליניארית לאורך השנים לכל מדינה (ללא apply על קבוצה שלמה)
clean = clean.sort_values(["Country","Year"]).copy()
for c in num_cols:
    clean[c] = (clean
                .groupby("Country")[c]
                .transform(lambda s: s.interpolate(method="linear", limit_direction="both")))

# 2) טיפול ממוקד בשאריות (בד"כ רק HLE)
hle = "Healthy_life_expectancy"
if hle in clean.columns:
    # חציון למדינה
    med_country = clean.groupby("Country")[hle].transform("median")
    clean[hle] = clean[hle].fillna(med_country)
    # חציון לשנה (אם במדינה אין בכלל נתונים)
    med_year = clean.groupby("Year")[hle].transform("median")
    clean[hle] = clean[hle].fillna(med_year)
    # חציון כולל (רשת ביטחון)
    clean[hle] = clean[hle].fillna(clean[hle].median())

# דו"ח חוסרים אחרי טיפול
print("חסרים אחרי טיפול (לפי עמודה):")
print(clean[num_cols].isna().sum())

""" שמירה טבלה מאוחד אחרי טיפול בחוסרים"""

##כיסוי שנים וספירת מדינות
print("שנים זמינות:", sorted(clean["Year"].dropna().unique().tolist()))
print("\nכמה רשומות לכל שנה:")
print(clean.groupby("Year").size())
print("\nכמה מדינות לכל שנה:")
print(clean.groupby("Year")["Country"].nunique())

# שמירה
out_csv = "world_happiness_2015_2024_clean.csv"
clean.to_csv(out_csv, index=False, encoding="utf-8")
print(f"\nSaved: {out_csv} | Shape: {clean.shape}")

"""## QC מסכם + תקציר לדו"ח

"""

# === תא 6: QC מסכם + תקציר לדו"ח האמצע ===

# 1) בדיקת כפילויות Country–Year
dup_mask = clean.duplicated(subset=["Country","Year"], keep=False)
dup_n = int(dup_mask.sum())
if dup_n > 0:
    print(f"[אזהרה] נמצאו {dup_n} כפילויות Country–Year. נשמור מופע ראשון ונפיל כפולים.")
    clean = clean.drop_duplicates(subset=["Country","Year"]).reset_index(drop=True)
else:
    print("[OK] אין כפילויות Country–Year.")

# 2) חסרים (באחוזים)
num_cols = ["Ladder_Score","GDP_per_capita","Social_support",
            "Healthy_life_expectancy","Freedom","Generosity","Corruption"]
num_cols = [c for c in num_cols if c in clean.columns]
missing_pct = (clean[num_cols].isna().mean().sort_values(ascending=False) * 100).round(2)
print("\nאחוזי חסרים לפי עמודה (%):")
display(missing_pct.to_frame("missing_pct"))

# 3) טווחי ערכים לכל מדד מרכזי
ranges = clean[num_cols].describe().T[["min","max","mean","std"]].round(3)
print("\nטווחי ערכים למדדים עיקריים:")
display(ranges)

# 4) Shape סופי
rows, cols = clean.shape
print(f"\n[OK] Shape סופי: {rows}×{cols}")

# 5) תקציר מוכן לדו"ח
years_cov = f"{int(clean['Year'].min())}–{int(clean['Year'].max())}"
avg_missing = round(float(clean[num_cols].isna().mean().mean() * 100), 2)

summary_text = f"""
נתונים (Data):
הפרויקט עושה שימוש במאגר World Happiness Report לשנים {years_cov}.
לאחר תהליך ETL של איחוד מקורות, יישור שמות וטיוב (כולל מילוי חסרים ליניארי לכל מדינה לאורך השנים),
התקבל קובץ מאוחד בגודל {rows}×{cols}.
בוצעו בדיקות איכות: כפילויות לא נמצאו/נוקו, חסרים טופלו, ונבדקו טווחי ערכים סבירים לכל המדדים.
ממוצע חסרים כולל לאחר הטיוב: ≈ {avg_missing}%.
הקובץ הנקי נשמר לשימוש בהמשך בשם: world_happiness_2015_2024_clean.csv.
""".strip()

print("\n--- תקציר לדו\"ח האמצע (אפשר להעתיק/להדביק) ---")
print(summary_text)

"""# שלב 1 — ניתוח מתאמים (Exploratory Data Analysis)

בשלב זה נבחן את הקשרים בין הפיצ'רים השונים לבין
.מדד האושר
המתאמים מאפשרים לנו לזהות אילו משתנים מסבירים יותר את השונות במדד האושר,
ולהכין את הקרקע למודלים חיזויים בהמשך.

טבלת מתאמים
"""

# === שלב 1: חישוב מתאמים עם Ladder_Score ===
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams["font.family"] = "DejaVu Sans"   # נתמך בקולאב וכולל עברית
mpl.rcParams["axes.unicode_minus"] = False    # סימן מינוס תקין

# בחירת רק את הפיצ'רים המספריים
num_cols = ["Ladder_Score","GDP_per_capita","Social_support",
            "Healthy_life_expectancy","Freedom","Generosity","Corruption"]
num_cols = [c for c in num_cols if c in clean.columns]

# חישוב מתאמים
corr = clean[num_cols].corr(method="pearson")

# הצגה טבלאית
print("טבלת מתאמים (Pearson):")
display(corr["Ladder_Score"].sort_values(ascending=False).to_frame("Correlation"))

# Heatmap ויזואלי
plt.figure(figsize=(8,6))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", center=0)
plt.title("Correlation matrix between variables")
plt.show()

"""גרף פיזור"""

# === Scatter: GDP per Capita vs. Ladder Score — גרסת דוח מלוטשת ===
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

# ── פרמטרים קטנים לשינוי מהיר ─────────────────────────────────────────
USE_LOG_X = False      # להפוך את ציר ה-GDP ללוג (True/False)
POINT_SIZE = 60        # גודל נקודה
POINT_ALPHA = 0.8      # שקיפות נקודה
MAX_LEGEND_ITEMS = 12  # אם יש יותר שנים מזה—נבטל מקרא כדי לשמור על ניקיון

# ── הכנת הדאטה ────────────────────────────────────────────────────────
df = clean.copy()
df["Year"] = df["Year"].astype(int)
df = df[["Country","Year","GDP_per_capita","Ladder_Score"]].dropna()

ymin, ymax = df["Year"].min(), df["Year"].max()

# שנה כקטגוריה + סדר מקרא עולה
year_order = sorted(df["Year"].unique())
df["Year_cat"] = pd.Categorical(df["Year"], categories=year_order, ordered=True).astype(str)

# ── גרף ───────────────────────────────────────────────────────────────
sns.set_theme(style="whitegrid")
sns.set_context("talk")  # טקסט גדול ונוח למצגת/דו"ח

fig, ax = plt.subplots(figsize=(9.5, 6.2))
sns.scatterplot(
    data=df,
    x="GDP_per_capita", y="Ladder_Score",
    hue="Year_cat", hue_order=[str(y) for y in year_order],
    palette="viridis", s=POINT_SIZE, alpha=POINT_ALPHA,
    edgecolor="white", linewidth=0.4, marker="o", ax=ax
)

# כותרות וצירים
ax.set_title(f"GDP per Capita vs. Ladder Score — {ymin}–{ymax}", pad=12)
ax.set_xlabel("GDP per Capita")
ax.set_ylabel("Ladder Score")

# פורמט אלפים לציר X (אם לא לוג)
if USE_LOG_X:
    ax.set_xscale("log")
else:
    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f"{int(x):,}"))

# שוליים קטנים כדי שלא ייחתך טקסט/נקודות בשוליים
ax.margins(x=0.03, y=0.03)
ax.grid(alpha=0.25)

# מקרא נקי: אם יותר מדי שנים—לא מציגים מקרא כדי למנוע עומס
handles, labels = ax.get_legend_handles_labels()
if len(labels) - 1 > MAX_LEGEND_ITEMS:  # (הפריט הראשון הוא כותרת המקרא)
    ax.legend([], [], frameon=False)
else:
    ax.legend(title="Year", bbox_to_anchor=(1.02, 1), loc="upper left", borderaxespad=0)

plt.tight_layout()
plt.show()

"""# שלב 2 — מודלי חיזוי והשוואת ביצועים

בשלב זה נבצע חיזוי של מדד האושר (Ladder_Score) באמצעות שני מודלים:
- רגרסיה ליניארית (Linear Regression)
- יער אקראי (Random Forest Regressor)

נשווה בין ביצועי המודלים על סט הבדיקה באמצעות מדדי MAE ו־R²,
ונשמור את התחזיות לקובץ לצורך המשך ניתוח והצגה.

"""

# === שלב 2: מודלי חיזוי והשוואת ביצועים ===
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# מאפיינים ומטרה
features = ["GDP_per_capita","Social_support","Healthy_life_expectancy",
            "Freedom","Generosity","Corruption"]
target   = "Ladder_Score"

df = clean.copy()

# פיצול ל־Train/Test לפי שנים: מאמנים עד 2022, בודקים על 2023–2024
if "Year" in df.columns and df["Year"].nunique() >= 5:
    years_sorted = sorted(df["Year"].dropna().unique())
    test_years = years_sorted[-2:]  # שנתיים אחרונות
    train = df[df["Year"] < min(test_years)]
    test  = df[df["Year"].isin(test_years)]
    if len(test) == 0:  # fallback אם אין מספיק
        train, test = train_test_split(df, test_size=0.2, random_state=42)
else:
    train, test = train_test_split(df, test_size=0.2, random_state=42)

X_train, y_train = train[features], train[target]
X_test,  y_test  = test[features],  test[target]

# Linear Regression pipeline
pipe_lr = Pipeline([
    ("imp", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
    ("lr", LinearRegression())
])

# Random Forest pipeline
pipe_rf = Pipeline([
    ("imp", SimpleImputer(strategy="median")),
    ("rf", RandomForestRegressor(n_estimators=300, random_state=42))
])

# אימון המודלים
pipe_lr.fit(X_train, y_train)
pipe_rf.fit(X_train, y_train)

# חיזוי
pred_lr = pipe_lr.predict(X_test)
pred_rf = pipe_rf.predict(X_test)

# מדדי ביצוע
print("=== Model Metrics (Test set) ===")
print("Linear Regression: MAE=", round(mean_absolute_error(y_test, pred_lr),3),
      " | R²=", round(r2_score(y_test, pred_lr),3))
print("Random Forest:     MAE=", round(mean_absolute_error(y_test, pred_rf),3),
      " | R²=", round(r2_score(y_test, pred_rf),3))

# טבלת תחזיות
pred_df = test[["Country","Year"]].copy()
pred_df["Actual_Ladder_Score"] = y_test.values
pred_df["Pred_LR"] = pred_lr
pred_df["Pred_RF"] = pred_rf

# שמירה לקובץ
out_csv = "happiness_predictions.csv"
pred_df.to_csv(out_csv, index=False, encoding="utf-8")
print(f"\nנשמר קובץ תחזיות: {out_csv}")
display(pred_df.head(10))

"""# שלב 3 — קלאסטרינג (KMeans) לפרופילי מדינות

מטרת הקלאסטרינג: לזהות קבוצות מדינות בעלות מאפיינים דומים במדדי ההסבר (GDP לנפש, תמיכה חברתית, תוחלת חיים בריאה, חירות, נדיבות ושחיתות),
ולהציג מפת מצב אינטואיטיבית.
"""

# === שלב 3: KMeans Clustering (מתוקן לפי המחברת + ויזואליזציה צבעונית עם כותרות) ===
# שינוי עיקרי: בחירת K עם Yellowbrick Elbow + בדיקת Silhouette, וטווח K=2..12
# הערה: אם זה ה-install הראשון של yellowbrick ב-Colab → הריצי את התא, אח"כ Runtime > Restart runtime, ואז הריצי שוב.

!pip -q install -U yellowbrick

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer
import matplotlib.pyplot as plt

# ===================== נתונים בסיסיים =====================
dfc = clean.copy()  # מניחים שקיימת טבלת clean אחרי ניקוי
features = ["GDP_per_capita","Social_support","Healthy_life_expectancy",
            "Freedom","Generosity","Corruption"]

# אימפוט + תקנון (כמו אצלך)
imp = SimpleImputer(strategy="median")
scaler = StandardScaler()
X_full = scaler.fit_transform(imp.fit_transform(dfc[features]))

# ===================== בחירת K (תיקון סביב K) =====================
K_MIN, K_MAX = 2, 12
RANDOM_STATE = 42

# 1) Elbow (Yellowbrick) – זיהוי "מרפק" בעקומת האינרציה
base_model = KMeans(n_init=10, random_state=RANDOM_STATE)
elbow_viz = KElbowVisualizer(base_model, k=(K_MIN, K_MAX), timings=False)
elbow_viz.fit(X_full)
elbow_k = elbow_viz.elbow_value_

# מציגים את גרף ה-Elbow (כמו במחברת של המרצה)
elbow_viz.show();

# 2) Silhouette – גיבוי/אימות
Ks = list(range(K_MIN, K_MAX+1))
sils = []
for k in Ks:
    km = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_STATE)
    labels = km.fit_predict(X_full)
    sils.append(silhouette_score(X_full, labels))

silhouette_k = Ks[int(np.argmax(sils))]

# 3) הכרעה שקופה: אם יש elbow ברור → נבחר בו; אחרת → ב-Max Silhouette
best_k = int(elbow_k) if elbow_k is not None else int(silhouette_k)

print("=== בחירת K ===")
print("Elbow (K):", elbow_k, "| Silhouette-best (K):", silhouette_k, "| Chosen K:", best_k)
print("Silhouette by K:", list(zip(Ks, [round(v,3) for v in sils])))

# ===================== אימון סופי והפקת אשכולות =====================
kmeans = KMeans(n_clusters=best_k, n_init=10, random_state=RANDOM_STATE)
dfc["Cluster"] = kmeans.fit_predict(X_full).astype(int)

# ===================== סיכום קצר (לבחירה) =====================
profiles = (
    dfc.groupby("Cluster")[features + ["Ladder_Score"]]
       .mean()
       .round(3)
       .sort_index()
)
sizes = dfc["Cluster"].value_counts().sort_index()

print("\n=== גודל אשכולות (מס' רשומות) ===")
display(sizes.to_frame("count"))
print("\n=== ממוצעי מאפיינים לפי אשכול (כולל Ladder_Score) ===")
display(profiles)

# ===================== ויזואליזציה צבעונית עם כותרות =====================
# הורדה ל-2 ממדים לויזואליזציה (PCA)
pca = PCA(n_components=2, random_state=RANDOM_STATE)
coords = pca.fit_transform(X_full)

# צבעים נעימים (tab10) + מקרא עם ספירת נקודות לכל אשכול
clusters = np.sort(dfc["Cluster"].unique())
n_clusters = len(clusters)
cmap = plt.get_cmap("tab10") if n_clusters <= 10 else plt.get_cmap("tab20")
colors = [cmap(i) for i in range(n_clusters)]
color_map = {c: colors[i] for i, c in enumerate(clusters)}
counts = dfc["Cluster"].value_counts().sort_index()

plt.figure(figsize=(9,6.8))
for c in clusters:
    mask = (dfc["Cluster"].values == c)
    plt.scatter(
        coords[mask, 0], coords[mask, 1],
        label=f"Cluster {c} (n={int(counts[c])})",
        alpha=0.8, s=55, c=[color_map[c]]
    )

# סימון מרכזים (במרחב PCA)
centers_2d = (
    pd.DataFrame({"x": coords[:,0], "y": coords[:,1], "Cluster": dfc["Cluster"].values})
      .groupby("Cluster")[["x","y"]].mean()
)
plt.scatter(
    centers_2d["x"], centers_2d["y"],
    marker="X", s=180, edgecolor="black", linewidths=1.2,
    c=[color_map[c] for c in centers_2d.index], label="Centroids"
)

# כותרות ותתי-כותרות
title_main = f"KMeans – PCA (2D)  |  K={best_k}"
subtitle = f"PCA1={pca.explained_variance_ratio_[0]*100:.1f}%,  PCA2={pca.explained_variance_ratio_[1]*100:.1f}% variance"
plt.title(title_main + "\n" + subtitle)
plt.xlabel("PCA1")
plt.ylabel("PCA2")
plt.legend(title="אשכולות", frameon=True, ncol=2)
plt.grid(alpha=0.25)
plt.tight_layout()
plt.show()

# ===================== יצוא נקי לפאוור BI =====================
out_clusters = "world_happiness_2015_2024_clusters.csv"
dfc[["Country","Year","Cluster"]].to_csv(out_clusters, index=False, encoding="utf-8")
print(f"\n\ נשמר קובץ אשכולות: {out_clusters}  (שורות: {len(dfc)})")
